{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Odds Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/kalkiek/projects/reddit-political-affiliation/')\n",
    "\n",
    "import re\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import bz2\n",
    "import lzma\n",
    "import json\n",
    "import zstandard as zstd\n",
    "from json import JSONDecodeError\n",
    "# from src.data.download_flair_data import parse_submissions, parse_zst_submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in User Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_month = '2019-01'\n",
    "\n",
    "in_file = '/shared/0/projects/reddit-political-affiliation/data/word2vec/predictions/users_' + year_month + '.tsv'\n",
    "user_predictions = {}\n",
    "\n",
    "with open(in_file, 'r') as f:\n",
    "    for line in f:\n",
    "        user, score = line.split('\\t')\n",
    "        user_predictions[user] = float(score.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the 'fringe' users from both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of right users: 4386802\n",
      "Number of left users: 419697\n"
     ]
    }
   ],
   "source": [
    "left, right = {}, {}\n",
    "\n",
    "for user, score in user_predictions.items():\n",
    "    if score >= 0.75:\n",
    "        right[user] = score\n",
    "    elif score <= 0.25:\n",
    "        left[user] = score\n",
    "        \n",
    "print(\"Number of right users: {}\".format(len(right)))\n",
    "print(\"Number of left users: {}\".format(len(left)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word frequencies for all fringe users from this month\n",
    "\n",
    "### Code to handle the raw (compressed) data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1000000 lines\n",
      "Completed 2000000 lines\n",
      "Completed 3000000 lines\n",
      "Completed 4000000 lines\n",
      "Completed 5000000 lines\n",
      "Completed 6000000 lines\n",
      "Completed 7000000 lines\n",
      "Completed 8000000 lines\n",
      "Completed 9000000 lines\n",
      "Completed 10000000 lines\n",
      "Completed 11000000 lines\n",
      "Completed 12000000 lines\n",
      "Completed 13000000 lines\n",
      "Completed 14000000 lines\n",
      "Completed 15000000 lines\n",
      "Completed 16000000 lines\n",
      "Completed 17000000 lines\n",
      "Completed 18000000 lines\n",
      "Completed 19000000 lines\n",
      "Completed 20000000 lines\n",
      "Completed 21000000 lines\n",
      "Completed 22000000 lines\n",
      "Completed 23000000 lines\n",
      "Completed 24000000 lines\n",
      "Completed 25000000 lines\n",
      "Completed 26000000 lines\n",
      "Completed 27000000 lines\n",
      "Completed 28000000 lines\n",
      "Completed 29000000 lines\n",
      "Completed 30000000 lines\n",
      "Completed 31000000 lines\n",
      "Completed 32000000 lines\n",
      "Completed 33000000 lines\n",
      "Completed 34000000 lines\n",
      "Completed 35000000 lines\n",
      "Completed 36000000 lines\n",
      "Completed 37000000 lines\n",
      "Completed 38000000 lines\n",
      "Completed 41000000 lines\n",
      "Completed 42000000 lines\n",
      "Completed 43000000 lines\n",
      "Completed 44000000 lines\n",
      "Completed 45000000 lines\n",
      "Completed 46000000 lines\n",
      "Completed 47000000 lines\n",
      "Completed 48000000 lines\n",
      "Completed 49000000 lines\n",
      "Completed 50000000 lines\n",
      "Completed 51000000 lines\n",
      "Completed 52000000 lines\n",
      "Completed 53000000 lines\n",
      "Completed 54000000 lines\n",
      "Completed 55000000 lines\n",
      "Completed 56000000 lines\n",
      "Completed 57000000 lines\n",
      "Completed 58000000 lines\n",
      "Completed 59000000 lines\n",
      "Completed 60000000 lines\n",
      "Completed 61000000 lines\n",
      "Completed 62000000 lines\n",
      "Completed 63000000 lines\n",
      "Completed 64000000 lines\n",
      "Failed to parse line: b'{\"author\":\"BiH-Kira\",\"author_created_utc\":1309821335,\"author_flair_background_color\":\"\",\"author_flair_css_class\":\"kel\",\"author_flair_richtext\":[],\"author_flair_template_id\":null,\"author_flair_text\":\"\",\"author_flair_text_color\":\"dark\",\"author_flair_type\":\"text\",\"author_fullname\":\"t2_5gvzz\",\"author_patreon_flair\":false,\"body\":\"Better conditions for the player? Are you insane? How will this poor multi billion dollar company Activision Blizzard pay their CEO 7 figures with 6 figure bonuses? They are a company, not a charity, m8.\",\"can_gild\":true,\"can_mod_post\":false,\"collapsed\":false,\"collapsed_reason\":null,\"controversiality\":0,\"created_utc\":1548979199,\"distinguished\":null,\"edited\":false,\"gilded\":0,\"gildings\":{\"gid_1\":0,\"gid_2\":0,\"gid_3\":0},\"id\":\"efhcvsb\",\"is_submitter\":false,\"link_id\":\"t3_alu9nt\",\"no_follow\":false,\"parent_id\":\"t1_efhaqaq\",\"permalink\":\"\\\\/r\\\\/hearthstone\\\\/comments\\\\/alu9nt\\\\/kibler_on_the_nerfs_evergreen_set_and_gennbaku\\\\/efhcvsb\\\\/\",\"removal_reason\":null,\"retrieved_on\":1552340917,\"score\":108,\"send_replies\":true,\"stickied\":false,\"subreddit\":\"hearthstone\",\"subreddit_id\":\"t5_2w31t\",\"subreddit_name_prefixed\":\"r\\\\/hearthstone\",\"subreddit_type\":\"public\"}\\n' with error: Expecting value: line 1 column 1 (char 0)\n",
      "Number of words from left users: 5125509\n",
      "Number of words from right users: 20859251\n"
     ]
    }
   ],
   "source": [
    "def get_file_handle(file_path):\n",
    "    ext = file_path.split('.')[-1]\n",
    "\n",
    "    if ext == \"bz2\":\n",
    "        return bz2.open(file_path)\n",
    "    elif ext == \"xz\":\n",
    "        return lzma.open(file_path)\n",
    "\n",
    "    raise AssertionError(\"Invalid extension for \" + file_path + \". Expecting bz2 or xz file\")\n",
    "    \n",
    "\n",
    "def get_word_frequencies(file_pointer, left_users, right_users):\n",
    "    left_word_freq, right_word_freq = Counter(), Counter()\n",
    "    \n",
    "    for count, line in enumerate(file_pointer):\n",
    "        try:\n",
    "            submission = json.loads(f.readline().strip())\n",
    "            username, text = submission['author'], submission['body']\n",
    "\n",
    "            if username in left_users:\n",
    "                for word in text.split(' '):\n",
    "                    left_word_freq[word] += 1\n",
    "            elif username in right_users:\n",
    "                for word in text.split(' '):\n",
    "                    right_word_freq[word] += 1\n",
    "\n",
    "        except (JSONDecodeError, AttributeError) as e:\n",
    "            print(\"Failed to parse line: {} with error: {}\".format(line, e))\n",
    "\n",
    "        if count % 1000000 == 0 and count > 0:\n",
    "            print(\"Completed %d lines\" % (count))\n",
    "\n",
    "    return left_word_freq, right_word_freq\n",
    "\n",
    "\n",
    "def get_word_frequencies_zst_files(fname):\n",
    "    # Will implement later\n",
    "    pass\n",
    "\n",
    "\n",
    "file_path = '/shared/2/datasets/reddit-dump-all/RC/RC_2019-01.xz'\n",
    "f = get_file_handle(file_path)\n",
    "left_counts, right_counts = get_word_frequencies(f, left, right)\n",
    "\n",
    "print(\"Number of words from left users: {}\".format(len(left_counts)))\n",
    "print(\"Number of words from right users: {}\".format(len(right_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the counts to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = '/shared/0/projects/reddit-political-affiliation/data/word2vec/log-odds/' + year_month + '_left.json'\n",
    "\n",
    "with open(out_file, 'w') as fp:\n",
    "    json.dump(dict(left_counts), fp)\n",
    "    \n",
    "out_file = '/shared/0/projects/reddit-political-affiliation/data/word2vec/log-odds/' + year_month + '_right.json'\n",
    "\n",
    "with open(out_file, 'w') as fp:\n",
    "    json.dump(dict(right_counts), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Odds Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadCounts(filename, min_count=0, stopwords=set()):\n",
    "    result = defaultdict(int)\n",
    "    word_counts = json.load(open(filename))\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_count and word not in stopwords:\n",
    "              result[word] = count\n",
    "    return result\n",
    "  \n",
    "def LoadStopwords(filename):\n",
    "    stopwords = set()\n",
    "    for line in open(filename):\n",
    "        for word in line.split():\n",
    "            if word:\n",
    "                stopwords.add(word)\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def ComputeLogOdds(counts1, counts2, prior):\n",
    "    sigmasquared = defaultdict(float)\n",
    "    sigma = defaultdict(float)\n",
    "    delta = defaultdict(float)\n",
    "\n",
    "    for word in prior.keys():\n",
    "        prior[word] = int(prior[word] + 0.5)\n",
    "\n",
    "    for word in counts2.keys():\n",
    "        counts1[word] = int(counts1[word] + 0.5)\n",
    "        if prior[word] == 0:\n",
    "            prior[word] = 1\n",
    "\n",
    "    for word in counts1.keys():\n",
    "        counts2[word] = int(counts2[word] + 0.5)\n",
    "        if prior[word] == 0:\n",
    "            prior[word] = 1\n",
    "\n",
    "    n1  = sum(counts1.values())\n",
    "    n2  = sum(counts2.values())\n",
    "    nprior = sum(prior.values())\n",
    "\n",
    "\n",
    "    for word in prior.keys():\n",
    "        if prior[word] > 0:\n",
    "            l1 = float(counts1[word] + prior[word]) / (( n1 + nprior ) - (counts1[word] + prior[word]))\n",
    "            l2 = float(counts2[word] + prior[word]) / (( n2 + nprior ) - (counts2[word] + prior[word]))\n",
    "            sigmasquared[word] =  1/(float(counts1[word]) + float(prior[word])) + 1/(float(counts2[word]) + float(prior[word]))\n",
    "            sigma[word] =  math.sqrt(sigmasquared[word])\n",
    "            delta[word] = ( math.log(l1) - math.log(l2) ) / sigma[word]\n",
    "\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Log Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/kalkiek/nltk_data'\n    - '/opt/anaconda/nltk_data'\n    - '/opt/anaconda/share/nltk_data'\n    - '/opt/anaconda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/kalkiek/nltk_data'\n    - '/opt/anaconda/nltk_data'\n    - '/opt/anaconda/share/nltk_data'\n    - '/opt/anaconda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-31d000b02e5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/kalkiek/nltk_data'\n    - '/opt/anaconda/nltk_data'\n    - '/opt/anaconda/share/nltk_data'\n    - '/opt/anaconda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SETTINGS\n",
    "first_file = '/shared/0/projects/reddit-political-affiliation/data/word2vec/log-odds/' + year_month + '_left.json'\n",
    "second_file = '/shared/0/projects/reddit-political-affiliation/data/word2vec/log-odds/' + year_month + '_right.json'\n",
    "min_count = 100\n",
    "stop = stopwords.words('english')\n",
    "prior = defaultdict(lambda:0)\n",
    "out_file = '/shared/0/projects/reddit-political-affiliation/data/word2vec/log-odds/' + year_month + '_results.tsv'\n",
    "\n",
    "counts1 = LoadCounts(first_file, min_count, stop)\n",
    "counts2 = LoadCounts(second_file, min_count, stop)\n",
    "prior = LoadCounts(args.prior, args.min_count, stopwords)\n",
    "\n",
    "delta = ComputeLogOdds(counts1, counts2, prior)\n",
    "\n",
    "for word, log_odds in sorted(delta.items(), key=lambda x: x[1]):\n",
    "    args.out_file.write(\"{}\\t{:.3f}\\n\".format(word, log_odds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
