{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from os.path import basename\n",
    "from collections import *\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "year = '2018'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One month of data for development testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21229b2fdaa4839a5e41501257dadf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Processing all files', max=1.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb83450c50547119ac6a6deb5c6b65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building vocab from file', max=19865760.0, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Length of vocab: 4227361\n",
      "User count: 4159637\n",
      "Subreddit count: 67724\n"
     ]
    }
   ],
   "source": [
    "directory = '/shared/0/projects/reddit-political-affiliation/data/bipartite-networks/' + year + '*_filtered.tsv'\n",
    "files = glob.glob(directory)[:1]\n",
    "vocab = set()\n",
    "\n",
    "# Target is the subreddit and context is the users\n",
    "user_context = defaultdict(set)\n",
    "all_subreddits = set()\n",
    "\n",
    "for fname in tqdm(files, desc='Processing all files'):\n",
    "        with open(fname, 'rt') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in tqdm(lines, position=1, desc='Building vocab from file'):\n",
    "            user, subreddit, freq = line[:-1].split('\\t')\n",
    "            vocab.add(user)\n",
    "            vocab.add(subreddit)\n",
    "            user_context[user].add(subreddit)\n",
    "            all_subreddits.add(subreddit)\n",
    "            \n",
    "            \n",
    "all_subreddits = list(all_subreddits)\n",
    "print(\"Length of vocab: \" + str(len(vocab)))\n",
    "print(\"User count: \" + str(len(user_context)))\n",
    "print(\"Subreddit count: \" + str(len(all_subreddits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = '/shared/0/projects/reddit-political-affiliation/data/bipartite-networks/' + year + '*_filtered.tsv'\n",
    "files = glob.glob(directory)\n",
    "vocab = set()\n",
    "\n",
    "# Target is the subreddit and context is the users\n",
    "user_context = defaultdict(set)\n",
    "all_subreddits = set()\n",
    "\n",
    "for fname in tqdm(files, desc='Processing all files'):\n",
    "        with open(fname, 'rt') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in tqdm(lines, position=1, desc='Building vocab from file'):\n",
    "            user, subreddit, freq = line[:-1].split('\\t')\n",
    "            vocab.add(user)\n",
    "            vocab.add(subreddit)\n",
    "            user_context[user].add(subreddit)\n",
    "            all_subreddits.add(subreddit)\n",
    "            \n",
    "all_subreddits = list(all_subreddits)\n",
    "print(\"Length of vocab: \" + str(len(vocab)))\n",
    "print(\"User count: \" + str(len(user_context)))\n",
    "print(\"Subreddit count: \" + str(len(all_subreddits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in political affliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75a56ea6b644a099ce9697002a2e174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=164.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User to politic counts: 7832\n",
      "[('unitedstates', Counter({'Republican': 36})), ('ixid', Counter({'Democrat': 77})), ('TheMG', Counter({'Democrat': 12})), ('MoosPalang', Counter({'Democrat': 2})), ('well_here_I_am', Counter({'Republican': 46})), ('madwilliamflint', Counter({'Republican': 3})), ('lannister80', Counter({'Democrat': 7})), ('dcgh96', Counter({'Republican': 12})), ('G-3-R', Counter({'Republican': 9})), ('Eat_The_Muffin', Counter({'Republican': 3}))]\n",
      "Saw political affiliations for 7775 users\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('/shared/0/projects/reddit-political-affiliation/data/flair-affiliations/20*.tsv')\n",
    "\n",
    "user_to_politic_counts = defaultdict(Counter)\n",
    "\n",
    "for fname in tqdm(files):\n",
    "    with open(fname, 'rt') as f:\n",
    "        for line in f:\n",
    "            user, politics, freq = line.split('\\t')\n",
    "            user_to_politic_counts[user][politics] += int(freq)\n",
    "            \n",
    "print(\"User to politic counts: \" + str(len(user_to_politic_counts)))\n",
    "print(list(user_to_politic_counts.items())[:10])\n",
    "\n",
    "\n",
    "user_to_politics = {}\n",
    "for u, pc in user_to_politic_counts.items():\n",
    "    if len(pc) > 1:\n",
    "        continue\n",
    "    user_to_politics[u] = list(pc.keys())[0]\n",
    "print('Saw political affiliations for %d users' % len(user_to_politics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, politics in user_to_politics.items():\n",
    "    if politics == \"Democrat\":\n",
    "        user_to_politics[user] = 0\n",
    "    else:\n",
    "        user_to_politics[user] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom dataset class for easier batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubredditUserDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, user_to_subreddits, all_subreddits, user_to_politics, \\\n",
    "                 num_negative_samples=5, max_users=-1):\n",
    "\n",
    "        self.pos_and_neg_samples = []\n",
    "        # Mappings to the embedding dimensions\n",
    "        self.user_to_idx = {}\n",
    "        self.subreddit_to_idx = {}        \n",
    "        \n",
    "        def get_sub_idx(subreddit):\n",
    "            if subreddit in self.subreddit_to_idx:\n",
    "                sub_idx = self.subreddit_to_idx[subreddit]\n",
    "            else:\n",
    "                sub_idx = len(self.subreddit_to_idx)\n",
    "                self.subreddit_to_idx[subreddit] = len(self.subreddit_to_idx)            \n",
    "            return sub_idx\n",
    "        \n",
    "        num_users = len(user_context) if max_users < 0 else max_users\n",
    "        #c = Counter()\n",
    "        for i, (user, subreddits) in enumerate(tqdm(user_context.items(), total=num_users)):\n",
    "            if i >= num_users:\n",
    "                break\n",
    "            \n",
    "            if user in user_to_politics:\n",
    "                politics = user_to_politics[user]\n",
    "            else:\n",
    "                politics = -1\n",
    "            #c[politics]+=1\n",
    "            \n",
    "            self.user_to_idx[user] = len(self.user_to_idx)\n",
    "            user_idx = self.user_to_idx[user]\n",
    "            \n",
    "            # Add all the positive samples\n",
    "            for subreddit in subreddits:\n",
    "                sub_idx = get_sub_idx(subreddit)\n",
    "                self.pos_and_neg_samples.append((np.array([user_idx, sub_idx]), politics, 1))\n",
    "                \n",
    "            # Choose fixed negative samples \n",
    "            neg = []\n",
    "            num_neg = len(subreddits)*num_negative_samples\n",
    "            # guard against super active users?\n",
    "            num_neg = min(num_neg, len(all_subreddits) - num_neg)\n",
    "            while len(neg) < num_neg:\n",
    "                sub = all_subreddits[random.randint(0, len(all_subreddits) - 1)]\n",
    "                if sub not in subreddits: # Check if also in neg?\n",
    "                    neg.append(sub)\n",
    "            for n in neg:\n",
    "                sub_idx = get_sub_idx(subreddit)\n",
    "                self.pos_and_neg_samples.append((np.array([user_idx, sub_idx]), politics, 0))\n",
    "        #print(c)\n",
    "    def num_users(self):\n",
    "        return len(self.user_to_idx)\n",
    "\n",
    "    def num_subreddits(self):\n",
    "        return len(self.subreddit_to_idx)    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pos_and_neg_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos_and_neg_samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77370c9fadb49f3affc755f9b4fd487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4159637.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "training_data = SubredditUserDataset(user_context, all_subreddits, user_to_politics, \n",
    "                                     max_users=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task User2Subreddit model that also predicts political affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User2Subreddit(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_users, emb_dimension, num_subreddits):\n",
    "        super(User2Subreddit, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(num_users, emb_dimension)\n",
    "        self.v_embeddings = nn.Embedding(num_subreddits, emb_dimension)\n",
    "        self.political_layer = nn.Linear(emb_dimension, 1)\n",
    "        self.init_emb()\n",
    "        \n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.political_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, user_id, subreddit_id, political_user_ids):\n",
    "        emb_u = self.u_embeddings(user_id)\n",
    "        emb_v = self.v_embeddings(subreddit_id)\n",
    "        #print(emb_u.shape)\n",
    "        #print(emb_v.shape)\n",
    "        \n",
    "        # reshape to support batch dot-product\n",
    "        #emb_u = emb_u.view(emb_u.shape[0], 1, emb_u.shape[1])\n",
    "        #emb_v = emb_v.view(emb_v.shape[0], emb_v.shape[1], 1)       \n",
    "        #score = torch.bmm(emb_u, emb_v)\n",
    "        \n",
    "        # This this seems like the fastest way to do batch dot product:\n",
    "        # https://github.com/pytorch/pytorch/issues/18027\n",
    "        score = (emb_u*emb_v).sum(-1)\n",
    "        \n",
    "        #print(score.shape)\n",
    "        score = torch.sigmoid(score)\n",
    "        \n",
    "        # If we have political users to predict for\n",
    "        if political_user_ids.sum() > 0:\n",
    "            emb_p = self.u_embeddings(political_user_ids)\n",
    "            political_predictions = self.political_layer(emb_p)\n",
    "            political_predictions = torch.sigmoid(political_predictions)\n",
    "        else:\n",
    "            political_predictions = None\n",
    "            \n",
    "        return score, political_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "\n",
    "device = torch.device(\"cuda:6\") # Check with GPU is free with nvidia-smi\n",
    "\n",
    "#model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "model = User2Subreddit(training_data.num_users(), EMBEDDING_DIM, training_data.num_subreddits()).to(device)\n",
    "\n",
    "# model = nn.DataParallel(model, device_ids=GPU_IDS)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "loss_function = nn.BCELoss()\n",
    "#pol_model = Political(EMBEDDING_DIM).to(device)\n",
    "\n",
    "#pol_optimizer = optim.AdamW(pol_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir scalar/word2vec  --port 8010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c993bc01a6497bbe647398d05b0039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=50.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae38de261c454f1386efdca9f074ac3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59597.28), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1000: 1.303576\n",
      "Loss at step 2000: 1.190714\n",
      "Loss at step 3000: 1.141858\n",
      "Loss at step 4000: 1.019509\n",
      "Loss at step 5000: 0.883069\n",
      "Loss at step 6000: 0.755527\n",
      "Loss at step 7000: 0.754547\n",
      "Loss at step 8000: 0.576015\n",
      "Loss at step 9000: 0.650929\n",
      "Loss at step 10000: 0.564770\n",
      "Loss at step 11000: 0.619074\n",
      "Loss at step 12000: 0.537853\n",
      "Loss at step 13000: 0.449081\n",
      "Loss at step 14000: 0.549803\n",
      "Loss at step 15000: 0.406726\n",
      "Loss at step 16000: 0.424104\n",
      "Loss at step 17000: 0.418756\n",
      "Loss at step 18000: 0.448778\n",
      "Loss at step 19000: 0.379684\n",
      "Loss at step 20000: 0.355999\n",
      "Loss at step 21000: 0.354347\n",
      "Loss at step 22000: 0.344518\n",
      "Loss at step 23000: 0.356950\n",
      "Loss at step 24000: 0.363686\n",
      "Loss at step 25000: 0.499905\n",
      "Loss at step 26000: 0.323146\n",
      "Loss at step 27000: 0.351469\n",
      "Loss at step 28000: 0.326195\n",
      "Loss at step 29000: 0.321812\n",
      "Loss at step 30000: 0.329887\n",
      "Loss at step 31000: 0.288581\n",
      "Loss at step 32000: 0.292715\n",
      "Loss at step 33000: 0.325919\n",
      "Loss at step 34000: 0.297657\n",
      "Loss at step 35000: 0.289652\n",
      "Loss at step 36000: 0.307905\n",
      "Loss at step 37000: 0.281030\n",
      "Loss at step 38000: 0.293345\n",
      "Loss at step 39000: 0.281043\n",
      "Loss at step 40000: 0.275510\n",
      "Loss at step 41000: 0.282383\n",
      "Loss at step 42000: 0.282533\n",
      "Loss at step 43000: 0.292923\n",
      "Loss at step 44000: 0.282991\n",
      "Loss at step 45000: 0.277980\n",
      "Loss at step 46000: 0.325874\n",
      "Loss at step 47000: 0.281856\n",
      "Loss at step 48000: 0.279698\n",
      "Loss at step 49000: 0.266700\n",
      "Loss at step 50000: 0.276932\n",
      "Loss at step 51000: 0.266786\n",
      "Loss at step 52000: 0.262208\n",
      "Loss at step 53000: 0.273955\n",
      "Loss at step 54000: 0.256810\n",
      "Loss at step 55000: 0.244729\n",
      "Loss at step 56000: 0.242435\n",
      "Loss at step 57000: 0.263941\n",
      "Loss at step 58000: 0.265121\n",
      "Loss at step 59000: 0.249524\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13829ff7215e45ac9b54722531796153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59597.28), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1000: 0.238878\n",
      "Loss at step 2000: 0.244191\n",
      "Loss at step 3000: 0.226319\n",
      "Loss at step 4000: 0.226505\n",
      "Loss at step 5000: 0.240329\n",
      "Loss at step 6000: 0.238354\n",
      "Loss at step 7000: 0.216779\n",
      "Loss at step 8000: 0.237228\n",
      "Loss at step 9000: 0.228516\n",
      "Loss at step 10000: 0.234125\n",
      "Loss at step 11000: 0.253471\n",
      "Loss at step 12000: 0.221960\n",
      "Loss at step 13000: 0.227634\n",
      "Loss at step 14000: 0.244345\n",
      "Loss at step 15000: 0.219512\n",
      "Loss at step 16000: 0.222610\n",
      "Loss at step 17000: 0.205084\n",
      "Loss at step 18000: 0.237933\n",
      "Loss at step 19000: 0.226117\n",
      "Loss at step 20000: 0.216706\n",
      "Loss at step 21000: 0.227124\n",
      "Loss at step 22000: 0.220710\n",
      "Loss at step 23000: 0.225798\n",
      "Loss at step 24000: 0.229004\n",
      "Loss at step 25000: 0.205975\n",
      "Loss at step 26000: 0.201250\n",
      "Loss at step 27000: 0.217797\n",
      "Loss at step 28000: 0.234789\n",
      "Loss at step 29000: 0.236761\n",
      "Loss at step 30000: 0.214761\n",
      "Loss at step 31000: 0.222077\n",
      "Loss at step 32000: 0.206324\n",
      "Loss at step 33000: 0.250539\n",
      "Loss at step 34000: 0.239995\n",
      "Loss at step 35000: 0.219567\n",
      "Loss at step 36000: 0.214670\n",
      "Loss at step 37000: 0.233970\n",
      "Loss at step 38000: 0.207888\n",
      "Loss at step 39000: 0.240549\n",
      "Loss at step 40000: 0.191402\n",
      "Loss at step 41000: 0.231969\n",
      "Loss at step 42000: 0.219173\n",
      "Loss at step 43000: 0.210892\n",
      "Loss at step 44000: 0.228091\n",
      "Loss at step 45000: 0.210869\n",
      "Loss at step 46000: 0.229394\n",
      "Loss at step 47000: 0.204928\n",
      "Loss at step 48000: 0.228826\n",
      "Loss at step 49000: 0.227709\n",
      "Loss at step 50000: 0.215565\n",
      "Loss at step 51000: 0.210109\n",
      "Loss at step 52000: 0.246672\n",
      "Loss at step 53000: 0.208919\n",
      "Loss at step 54000: 0.222684\n",
      "Loss at step 55000: 0.214335\n",
      "Loss at step 56000: 0.208391\n",
      "Loss at step 57000: 0.249905\n",
      "Loss at step 58000: 0.215560\n",
      "Loss at step 59000: 0.219829\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a87b8652b14a81a036ff83eee3687c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59597.28), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1000: 0.204851\n",
      "Loss at step 2000: 0.209224\n",
      "Loss at step 3000: 0.195189\n",
      "Loss at step 4000: 0.201477\n",
      "Loss at step 5000: 0.217809\n",
      "Loss at step 6000: 0.177617\n",
      "Loss at step 7000: 0.214686\n",
      "Loss at step 8000: 0.201794\n",
      "Loss at step 9000: 0.186397\n",
      "Loss at step 10000: 0.230329\n",
      "Loss at step 11000: 0.195911\n",
      "Loss at step 12000: 0.195249\n",
      "Loss at step 13000: 0.207946\n",
      "Loss at step 14000: 0.213625\n",
      "Loss at step 15000: 0.212532\n",
      "Loss at step 16000: 0.198463\n",
      "Loss at step 17000: 0.225018\n",
      "Loss at step 18000: 0.196940\n",
      "Loss at step 19000: 0.205046\n",
      "Loss at step 20000: 0.182587\n",
      "Loss at step 21000: 0.219257\n",
      "Loss at step 22000: 0.205601\n",
      "Loss at step 23000: 0.215484\n",
      "Loss at step 24000: 0.211120\n",
      "Loss at step 25000: 0.214080\n",
      "Loss at step 26000: 0.200246\n",
      "Loss at step 27000: 0.199296\n",
      "Loss at step 28000: 0.209348\n",
      "Loss at step 29000: 0.207838\n",
      "Loss at step 30000: 0.234481\n",
      "Loss at step 31000: 0.204855\n",
      "Loss at step 32000: 0.200574\n",
      "Loss at step 33000: 0.231395\n",
      "Loss at step 34000: 0.225097\n",
      "Loss at step 35000: 0.191758\n",
      "Loss at step 36000: 0.212490\n",
      "Loss at step 37000: 0.195794\n",
      "Loss at step 38000: 0.195216\n",
      "Loss at step 39000: 0.216339\n",
      "Loss at step 40000: 0.194929\n",
      "Loss at step 41000: 0.211247\n",
      "Loss at step 42000: 0.207600\n",
      "Loss at step 43000: 0.206995\n",
      "Loss at step 44000: 0.195201\n",
      "Loss at step 45000: 0.229203\n",
      "Loss at step 46000: 0.197135\n",
      "Loss at step 47000: 0.197710\n",
      "Loss at step 48000: 0.206403\n",
      "Loss at step 49000: 0.212885\n",
      "Loss at step 50000: 0.205758\n",
      "Loss at step 51000: 0.205093\n",
      "Loss at step 52000: 0.217131\n"
     ]
    }
   ],
   "source": [
    "batch_size=2000\n",
    "EPOCHS = 50\n",
    "\n",
    "dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "writer = SummaryWriter(logdir='/shared/0/projects/reddit-political-affiliation/tensorboard-logs/')\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epoch'):\n",
    "    total_loss, pol_loss = 0, 0\n",
    "\n",
    "    for i, data in enumerate(tqdm(dataloader, total=len(training_data)/batch_size), 1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        user_sub, politics_labels, subreddit_labels = data    \n",
    "        \n",
    "        # print(politics_labels)\n",
    "\n",
    "        user_ids = user_sub[:,0]\n",
    "        subreddit_ids = user_sub[:,1].to(device)\n",
    "        \n",
    "        # Grab the user IDs for those that had political labels\n",
    "        p_indices = [ i for i, v in enumerate(politics_labels) if v >= 0 ]\n",
    "        political_ids = user_ids.index_select(0, torch.LongTensor(p_indices))\n",
    "        \n",
    "        user_ids = user_ids.to(device)\n",
    "        political_ids = political_ids.to(device)\n",
    "        \n",
    "        subreddit_preds, pol_preds = model(user_ids, subreddit_ids, political_ids)\n",
    "        \n",
    "        #print(preds.shape)\n",
    "        #print(labels.shape)\n",
    "        #print('preds: ', preds)\n",
    "        #print('labes: ', labels)\n",
    "        \n",
    "        subreddit_labels = subreddit_labels.float().to(device)\n",
    "        \n",
    "        loss = loss_function(subreddit_preds, subreddit_labels)\n",
    "        \n",
    "        # If we had some political users in this batch...\n",
    "        if len(p_indices) > 0:\n",
    "            pol_labels = torch.LongTensor([ v for v in politics_labels if v >= 0 ]).float().to(device)        \n",
    "            \n",
    "            # Squeeze call necessary to go from (k, 1) to (k) dimensions due to batching\n",
    "            pol_loss = loss_function(pol_preds.squeeze(), pol_labels)\n",
    "            \n",
    "            #print(pol_labels.shape, pol_preds.shape)\n",
    "            \n",
    "            writer.add_scalar('political loss', pol_loss.cpu().detach().numpy(),\n",
    "                              i*batch_size + epoch*len(training_data))\n",
    "            loss += pol_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        writer.add_scalar('word2vec loss', loss.cpu().detach().numpy(),\n",
    "                          i*batch_size + epoch*len(training_data))\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print('Loss at step %d: %f' % (i, loss.cpu().detach().numpy()))\n",
    "            \n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "losses = []\n",
    "writer = SummaryWriter(logdir='scalar/word2vec')\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epoch'):\n",
    "    total_loss, pol_loss = 0, 0\n",
    "\n",
    "    for subreddits, user in tqdm(context_vecs, desc='Processing subreddits for user'):\n",
    "        context_ids = make_context_vector(subreddits, word_to_ix).to(device)\n",
    "        \n",
    "        out_act = model(context_ids)\n",
    "        \n",
    "        # Generate 2 negative samples for every positive sample\n",
    "        negative_samples = generate_negative_samples(user, len(subreddits) * 2)\n",
    "        negative_ids = make_context_vector(negative_samples, word_to_ix).to(device)\n",
    "        \n",
    "        loss = loss_function(out_act, torch.tensor([word_to_ix[user]], dtype=torch.long))\n",
    "        \n",
    "        # Update loss function\n",
    "        for sub_ix in context_ids:\n",
    "            loss += 1 - torch.sigmoid(out_act[0, sub_ix]) \n",
    "\n",
    "        for sub_ix in negative_ids:\n",
    "            loss += 0 - torch.sigmoid(out_act[0, sub_ix])\n",
    "            \n",
    "        # If we know their political affiliation pass it through another linear layer\n",
    "        if user in user_to_politics:\n",
    "            \n",
    "            pred = pol_model(torch.tensor([word_to_ix[user]], dtype=torch.long))\n",
    "            pol_loss = loss_function(pred, user_to_politics[user])\n",
    "            # TODO: Review this\n",
    "            loss += pol_loss\n",
    "            pol_loss.backward()\n",
    "            pol_optimizer.step()\n",
    "                             \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         writer.add_scalar('word2vec loss', loss.detach().numpy(), epoch)     \n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "writer.close()\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_similar(subreddit, n):\n",
    "    cosine_sims = {}\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    sub_tensor = lookup_tensor = torch.tensor([word_to_ix[subreddit]], dtype=torch.long)\n",
    "\n",
    "    for sub, _ in top_subs.items():\n",
    "        lookup_tensor = torch.tensor([word_to_ix[sub]], dtype=torch.long)\n",
    "        result = cos(model.embeddings(sub_tensor), model.embeddings(lookup_tensor))\n",
    "        cosine_sims[sub] = result\n",
    "        \n",
    "    cosine_sims = {k: v for k, v in sorted(cosine_sims.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return dict(itertools.islice(cosine_sims.items(), n))\n",
    "    \n",
    "top_n_similar('r/CryptoCurrency', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embeddings to TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Political Affiliation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
