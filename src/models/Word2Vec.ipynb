{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from os.path import basename\n",
    "from collections import *\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "year = '2018'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One month of data for development testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc65feac3c540fa81ce732be6d0a63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Processing all files', max=1.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de201998adf04602812bf6ddc959ec12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Building vocab from file', max=19865760.0, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Length of vocab: 4227361\n",
      "User count: 4159637\n",
      "Subreddit count: 67724\n"
     ]
    }
   ],
   "source": [
    "directory = '/shared/0/projects/reddit-political-affiliation/data/bipartite-networks/' + year + '*_filtered.tsv'\n",
    "files = glob.glob(directory)[:1]\n",
    "vocab = set()\n",
    "\n",
    "# Target is the subreddit and context is the users\n",
    "user_context = defaultdict(set)\n",
    "all_subreddits = set()\n",
    "\n",
    "for fname in tqdm(files, desc='Processing all files'):\n",
    "        with open(fname, 'rt') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in tqdm(lines, position=1, desc='Building vocab from file'):\n",
    "            user, subreddit, freq = line[:-1].split('\\t')\n",
    "            vocab.add(user)\n",
    "            vocab.add(subreddit)\n",
    "            user_context[user].add(subreddit)\n",
    "            all_subreddits.add(subreddit)\n",
    "            \n",
    "            \n",
    "all_subreddits = list(all_subreddits)\n",
    "print(\"Length of vocab: \" + str(len(vocab)))\n",
    "print(\"User count: \" + str(len(user_context)))\n",
    "print(\"Subreddit count: \" + str(len(all_subreddits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire year of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = '/shared/0/projects/reddit-political-affiliation/data/bipartite-networks/' + year + '*_filtered.tsv'\n",
    "files = glob.glob(directory)\n",
    "vocab = set()\n",
    "\n",
    "# Target is the subreddit and context is the users\n",
    "user_context = defaultdict(set)\n",
    "all_subreddits = set()\n",
    "\n",
    "for fname in tqdm(files, desc='Processing all files'):\n",
    "        with open(fname, 'rt') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in tqdm(lines, position=1, desc='Building vocab from file'):\n",
    "            user, subreddit, freq = line[:-1].split('\\t')\n",
    "            vocab.add(user)\n",
    "            vocab.add(subreddit)\n",
    "            user_context[user].add(subreddit)\n",
    "            all_subreddits.add(subreddit)\n",
    "            \n",
    "all_subreddits = list(all_subreddits)\n",
    "print(\"Length of vocab: \" + str(len(vocab)))\n",
    "print(\"User count: \" + str(len(user_context)))\n",
    "print(\"Subreddit count: \" + str(len(all_subreddits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in political affliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d398e52562405fbc5eae7ab0e5db09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=164.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User to politic counts: 7832\n",
      "[('unitedstates', Counter({'Republican': 36})), ('ixid', Counter({'Democrat': 77})), ('TheMG', Counter({'Democrat': 12})), ('MoosPalang', Counter({'Democrat': 2})), ('well_here_I_am', Counter({'Republican': 46})), ('madwilliamflint', Counter({'Republican': 3})), ('lannister80', Counter({'Democrat': 7})), ('dcgh96', Counter({'Republican': 12})), ('G-3-R', Counter({'Republican': 9})), ('Eat_The_Muffin', Counter({'Republican': 3}))]\n",
      "Saw political affiliations for 7775 users\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('/shared/0/projects/reddit-political-affiliation/data/flair-affiliations/20*.tsv')\n",
    "\n",
    "user_to_politic_counts = defaultdict(Counter)\n",
    "\n",
    "for fname in tqdm(files):\n",
    "    with open(fname, 'rt') as f:\n",
    "        for line in f:\n",
    "            user, politics, freq = line.split('\\t')\n",
    "            user_to_politic_counts[user][politics] += int(freq)\n",
    "            \n",
    "print(\"User to politic counts: \" + str(len(user_to_politic_counts)))\n",
    "print(list(user_to_politic_counts.items())[:10])\n",
    "\n",
    "\n",
    "user_to_politics = {}\n",
    "for u, pc in user_to_politic_counts.items():\n",
    "    if len(pc) > 1:\n",
    "        continue\n",
    "    user_to_politics[u] = list(pc.keys())[0]\n",
    "print('Saw political affiliations for %d users' % len(user_to_politics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, politics in user_to_politics.items():\n",
    "    if politics == \"Democrat\":\n",
    "        user_to_politics[user] = 0\n",
    "    else:\n",
    "        user_to_politics[user] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec\n",
    "\n",
    "Subreddits are the context and users are the target i.e. users2subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c9c56d333a482b98bba142a180b428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4159637.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[(['r/tifu', 'r/forwardsfromgrandma', 'r/dontdeadopeninside', 'r/badlinguistics', 'r/woahdude', 'r/Minecraft', 'r/assholedesign', 'r/meirl', 'r/civ', 'r/tumblr'], 'firedrake242'), (['r/communism', 'r/math_irl', 'r/esist', 'r/DnDGreentext', 'r/hmmm', 'r/history', 'r/translator', 'r/self', 'r/gifs', 'r/mildlyinteresting'], 'firedrake242')]\n"
     ]
    }
   ],
   "source": [
    "# Build bag of words context vectors\n",
    "CONTEXT_SIZE = 10\n",
    "context_vecs = []\n",
    "vocab = set()\n",
    "\n",
    "for user, subs in tqdm(user_context.items()):\n",
    "    subs = list(subs)\n",
    "    vocab.add(user)\n",
    "    [vocab.add(s) for s in subs]\n",
    "    for i in range(0, len(subs) - CONTEXT_SIZE, CONTEXT_SIZE):\n",
    "        context = (subs[i:i+CONTEXT_SIZE], user)\n",
    "        context_vecs.append(context)\n",
    "        \n",
    "print(context_vecs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(user, n):\n",
    "    samples = []\n",
    "    \n",
    "    while len(samples) < n:\n",
    "        rand_index = random.randint(0, len(all_subreddits) - 1)\n",
    "        sub = all_subreddits[rand_index]\n",
    "        \n",
    "        if sub not in user_context[user]:\n",
    "            samples.append(sub)\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size * context_size, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        hidden = self.linear1(embeds)\n",
    "        out = F.relu(hidden)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Think of a better name for this\n",
    "class Political(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size):\n",
    "        super(Political, self).__init__()\n",
    "        self.linear = nn.Linear(embedding_size, 1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.view((1, -1))\n",
    "        out = self.linear(inputs)\n",
    "        return nn.Sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "# model = nn.DataParallel(model, device_ids=GPU_IDS)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "loss_function = nn.NLLLoss()\n",
    "pol_model = Political(EMBEDDING_DIM)\n",
    "pol_optimizer = optim.SGD(pol_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6546 (pid 85105), started 0:04:36 ago. (Use '!kill 85105' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9fa154b6e2563d93\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9fa154b6e2563d93\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6546;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir scalar/word2vec # --port 6546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68293650ae524de6acf0fa4631dd8fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d56f5ec6bfd40019190466d6dc43b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Processing subreddits for user', max=719347.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-c7f87e5cedf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mpol_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#         writer.add_scalar('word2vec loss', loss.detach().numpy(), epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "losses = []\n",
    "writer = SummaryWriter(logdir='scalar/word2vec')\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc='Epoch'):\n",
    "    total_loss, pol_loss = 0, 0\n",
    "\n",
    "    for subreddits, user in tqdm(context_vecs, desc='Processing subreddits for user'):\n",
    "        context_ids = make_context_vector(subreddits, word_to_ix)\n",
    "        out_act = model(context_ids)\n",
    "        \n",
    "        # Generate 2 negative samples for every positive sample\n",
    "        negative_samples = generate_negative_samples(user, len(subreddits) * 2)\n",
    "        negative_ids = make_context_vector(negative_samples, word_to_ix)\n",
    "        loss = loss_function(out_act, torch.tensor([word_to_ix[user]], dtype=torch.long))\n",
    "        \n",
    "        # Update loss function\n",
    "        for sub_ix in context_ids:\n",
    "            loss += 1 - torch.sigmoid(out_act[0, sub_ix]) \n",
    "\n",
    "        for sub_ix in negative_ids:\n",
    "            loss += 0 - torch.sigmoid(out_act[0, sub_ix])\n",
    "            \n",
    "        # If we know their political affiliation pass it through another linear layer\n",
    "        if user in user_to_politics:\n",
    "            \n",
    "            pred = pol_model(torch.tensor([word_to_ix[user]], dtype=torch.long))\n",
    "            pol_loss = loss_function(pred, user_to_politics[user])\n",
    "            # TODO: Review this\n",
    "            loss += pol_loss\n",
    "            pol_loss.backward()\n",
    "            pol_optimizer.step()\n",
    "                             \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         writer.add_scalar('word2vec loss', loss.detach().numpy(), epoch)     \n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "writer.close()\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_similar(subreddit, n):\n",
    "    cosine_sims = {}\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    sub_tensor = lookup_tensor = torch.tensor([word_to_ix[subreddit]], dtype=torch.long)\n",
    "\n",
    "    for sub, _ in top_subs.items():\n",
    "        lookup_tensor = torch.tensor([word_to_ix[sub]], dtype=torch.long)\n",
    "        result = cos(model.embeddings(sub_tensor), model.embeddings(lookup_tensor))\n",
    "        cosine_sims[sub] = result\n",
    "        \n",
    "    cosine_sims = {k: v for k, v in sorted(cosine_sims.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return dict(itertools.islice(cosine_sims.items(), n))\n",
    "    \n",
    "top_n_similar('r/CryptoCurrency', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embeddings to TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Political Affiliation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
